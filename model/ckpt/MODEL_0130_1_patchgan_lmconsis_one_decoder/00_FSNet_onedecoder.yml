is_lm_debug: False
is_trainer_debug: True

train:
  lr: 0.00006       # init lr
  beta_1: 0.5
  beta_2: 0.999
  max_iters: 400000
  save_interval: 5000
  lr_decay_step: 2500
  lr_decay_weight: 0.9
  lambda_meanface_loss: 0.1     # not use

dis:                          # GAN discriminator
  nf: 64                      # base number of filters
  n_res_blks: 4              # number of residual blocks in the discriminator
  num_classes: 1            # number of classes in the training set

face:                        # face landmark detection model
  modelfilename: '/xxx_absolute_path_xxx/.../face_lm/trained_face_models/05_98_smallscale.pth1'     # path to the face landmark detection pretrained model, use absolute path
  nStack: 1         # number of stacks in face lm model
  heatmap_size: 64.0        # size of heatmap
  device: 'cuda'
  padding: 0
  norm2one: True            # whether to normalize the landmarks in range of 0 to 1
  num_classes: 98       # number of landmarks on a single face


init: kaiming               # initialization method, [gaussian/kaiming/xavier/orthogonal]

weights:        # balance different losses
  gan_w: 0.03                    # weight of adversarial loss
  fm_w: 0.03                       # weight on distance between gan features of real and generated images
  face_rec: 12.0             # weight on L1 distance between input image and reconstructed face
  lm_rec: 6.0            # weight on L1 distance between input landmark and reconstructed landmark
  lm2face2lm_rec: 0.1            # weight on DSNTNN loss between input landmark and reconstructed face landmark
  mean_face: 4.0             # weight on L1 distance between fixed mean face landmark and output of landmark encoder



trans:
  is_transloss: 0
  is_lm_transloss: 0
  is_lm2face2lm_loss: 0

gen:
  is_deepdecoder: 1         # whether to use deepere decoder in generator
  one_decoder_id: 4
  is_distribued_loss: 0
  is_fullconv: 0
  is_multi_scale_loss: 0

lm_AE:                      # landmark autoencoder
  is_meanface_mainroad: 1
  is_mean_lm_id_loss: 1
  is_hm_AE: 0
  is_hm_AE_v2: 0
  is_lm_ae_conv: 1          # whether to use conv structure in lm autoencoder, 0 means MLP structure
  is_nobd: 1                # donot use landmarks of face boundary
  is_pretrained_lm_encoder: 0       # whether to use pretrained landmark autoencoder
  mean_lm_id_root: '../datasets/aligned_images/CelebV'
#  mean_lm_id_root: ''
  margin_mean_general: 0.001
  margin_mean_id: 0.001






size: 220
padding: 18
optim: 'Adam'
is_apex: 0          # whther support apex training, not work when involved face_lm model
ig_curve: 800       # number of iterations at the beginning not added to log file
weight_decay: 0.0001
sshow: False
